name: CI

on:
  push:
    branches:
      - main
    paths:
      - 'airflow/**'
  pull_request:
    branches:
      - main
    paths:
      - 'airflow/**'
  workflow_dispatch:

env:
  AIRFLOW_BASE_URL: "http://localhost:8080"
  AWS_DEFAULT_REGION: "ap-southeast-2"
  AWS_REGION: "ap-southeast-2"
  EMR_MASTER_SECURITY_GROUP_ID: "sg-0cb660a18340f7dd3"
  ENABLE_REMOTE_LOGGING: "False"
  POSTGRES_DB: "airflow"
  POSTGRES_HOST: "postgres"
  POSTGRES_PASSWORD: "airflow"
  POSTGRES_PORT: "5432"
  POSTGRES_USER: "airflow"
  PROD_GLUE_DATABASE: "job"
  GLUE_CATALOG: "glue"
  GLUE_DATABASE: "job_ci"
  STAGE: "ci"       

jobs:
  build_test:
    environment: dev
    name: Build and test image
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up .env file
        run: |
          touch .env
          echo  AIRFLOW_BASE_URL=${{ env.AIRFLOW_BASE_URL }} >> .env
          echo  AWS_DEFAULT_REGION=${{ env.AWS_DEFAULT_REGION }} >> .env
          echo  AWS_REGION=${{ env.AWS_REGION }} >> .env
          echo  EMR_MASTER_SECURITY_GROUP_ID${{ env.EMR_MASTER_SECURITY_GROUP_ID }} >> .env
          echo  ENABLE_REMOTE_LOGGING=${{ env.ENABLE_REMOTE_LOGGING }} >> .env
          echo  POSTGRES_DB=${{ env.POSTGRES_DB }} >> .env
          echo  POSTGRES_HOST=${{ env.POSTGRES_HOST }} >> .env
          echo  POSTGRES_PORT=${{ env.POSTGRES_PORT }} >> .env
          echo  POSTGRES_USER=${{ env.POSTGRES_USER }} >> .env
          echo  PROD_GLUE_DATABASE=${{ env.PROD_GLUE_DATABASE }} >> .env
          echo  GLUE_CATALOG=${{ env.GLUE_CATALOG }} >> .env
          echo  GLUE_DATABASE=${{ env.GLUE_DATABASE }} >> .env
          echo  STAGE=${{ env.STAGE }} >> .env
          echo  AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }} >> .env
          echo  AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }} >> .env
          echo  FERNET_KEY=${{ secrets.FERNET_KEY }} >> .env
          echo  GLUE_DATABASE_STORAGE_LOCATION=${{ secrets.GLUE_DATABASE_STORAGE_LOCATION }} >> .env
          cat .env

      - name: Build image and start airflow containers
        run: |
          docker-compose up --build -d

      - name: Wait for airflow webserver to be healthy
        run: |
          HEALTH=""
          while [ "$HEALTH" != "healthy" ]; do
            HEALTH=$(docker inspect --format='{{.State.Health.Status}}' job_listing_webserver_1)
            if [ "$HEALTH" != "healthy" ]; then\
              echo "Waiting for airflow webserver to become healthy..."
              sleep 10
            fi
          done
      - name: Set up airflow connections
        run: |
          docker exec job_listing_webserver_1 airflow connections add 'adzuna_conn' --conn-json '{"conn_type": "http", "extra": {"application_id": "${{ secrets.ADZUNA_CONN_APPLICATION_ID }}", "application_password": "${{ secrets.ADZUNA_CONN_APPLICATION_PASSWORD }}"}}'
          docker exec job_listing_webserver_1 airflow connections add 'aws_custom' --conn-json '{"conn_type": "aws", "login":"${{ secrets.AWS_CUSTOM_LOGIN }}", "password":"${{ secrets.AWS_CUSTOM_PASSWORD }}", "extra": {"region_name": "ap-southeast-2"}}'
          docker exec job_listing_webserver_1 airflow connections add 'openai_conn'  --conn-json '{"conn_type": "http", "password":"${{ secrets.OPENAI_CONN_PASSWORD }}"}'
          docker exec job_listing_webserver_1 airflow connections add 'slack_conn' --conn-json '{"conn_type": "slack", "password":"${{ secrets.SLACK_CONN_PASSWORD }}"}'

      - name: Grant host machine access to EMR cluster
        run: |
          ip=$(curl ifconfig.me)
          echo $ip
          docker exec job_listing_webserver_1 sh -c "cd infra && python3 -m update_security_group_rules ${{ env.EMR_MASTER_SECURITY_GROUP_ID }} $ip grant"

      - name: Set up database
        run: |
          docker exec job_listing_webserver_1 sh -c "cd infra && python3 -m glue_database_set_up"

      - name: Run DAG validation and definition tests
        run: |
          docker exec job_listing_webserver_1 python3 -m pytest -vv tests/

      - name: Run job_listing DAG
        run: |
          docker exec job_listing_webserver_1 airflow dags unpause job_listing
          sleep 10

          job_listing_dag_run_status='queued'
          while [ "$job_listing_dag_run_status" == "running" -o "$job_listing_dag_run_status" == "queued" ]; do
            job_listing_dag_run_status=$(docker exec job_listing_webserver_1 airflow dags list-runs -d job_listing | grep -oE 'running|queued|success|failed')
            echo "running job_listing"
            sleep 60
          done

          echo "job_listing DAG run "$job_listing_dag_run_status
          if [ "$job_listing_dag_run_status" == "failed" ]; then
            exit 1
          fi

      - name: Revoke host machine access to EMR cluster
        run: |
          ip=$(curl ifconfig.me)
          docker exec job_listing_webserver_1 sh -c "cd infra && python3 -m update_security_group_rules ${{ env.EMR_MASTER_SECURITY_GROUP_ID }} $ip revoke"

      - name: Tear down database
        run: |
          docker exec job_listing_webserver_1 sh -c "cd infra && python3 -m glue_database_tear_down"

      - name: Stop airflow containers
        run: |
          docker-compose down