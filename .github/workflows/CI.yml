name: CI

on: [pull_request, workflow_dispatch, push]

env:
  AIRFLOW_BASE_URL: "http://localhost:8080"
  AWS_DEFAULT_REGION: "ap-southeast-2"
  AWS_REGION: "ap-southeast-2"
  EMR_MASTER_SECURITY_GROUP_ID: "sg-0cb660a18340f7dd3"
  ENABLE_REMOTE_LOGGING: "False"
  ENV: "dev"
  POSTGRES_DB: "airflow"
  POSTGRES_HOST: "postgres"
  POSTGRES_PASSWORD: "airflow"
  POSTGRES_PORT: "5432"
  POSTGRES_USER: "airflow"
  GLUE_CATALOG: "glue"
  GLUE_DATABASE: "job"
  STAGE: "dev"       

jobs:
  build_test:
    environment: dev
    name: Build and test image
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up .env file
        run: |
          touch airflow/.env
          echo  AIRFLOW_BASE_URL=${{ env.AIRFLOW_BASE_URL }} >> airflow/.env
          echo  AWS_DEFAULT_REGION=${{ env.AWS_DEFAULT_REGION }} >> airflow/.env
          echo  AWS_REGION=${{ env.AWS_REGION }} >> airflow/.env
          echo  EMR_MASTER_SECURITY_GROUP_ID${{ env.EMR_MASTER_SECURITY_GROUP_ID }} >> airflow/.env
          echo  ENABLE_REMOTE_LOGGING=${{ env.ENABLE_REMOTE_LOGGING }} >> airflow/.env
          echo  ENV=${{ env.ENV }} >> airflow/.env
          echo  POSTGRES_DB=${{ env.POSTGRES_DB }} >> airflow/.env
          echo  POSTGRES_HOST=${{ env.POSTGRES_HOST }} >> airflow/.env
          echo  POSTGRES_PORT=${{ env.POSTGRES_PORT }} >> airflow/.env
          echo  POSTGRES_USER=${{ env.POSTGRES_USER }} >> airflow/.env
          echo  GLUE_CATALOG=${{ env.GLUE_CATALOG }} >> airflow/.env
          echo  GLUE_DATABASE=${{ env.GLUE_DATABASE }} >> airflow/.env
          echo  STAGE=${{ env.STAGE }} >> airflow/.env
          echo  AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }} >> airflow/.env
          echo  AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }} >> airflow/.env
          echo  FERNET_KEY=${{ secrets.FERNET_KEY }} >> airflow/.env
          echo  GLUE_DATABASE_STORAGE_LOCATION=${{ secrets.GLUE_DATABASE_STORAGE_LOCATION }} >> airflow/.env
          cat airflow/.env

      - name: set Docker directory permission for cache
        run: |
          sudo chmod -R 777 /var/lib/docker

      - name: Cache image layers
        uses: actions/cache@v4
        with:
          path: /var/lib/docker
          key: docker-${{ hashFiles('**/Dockerfile') }}-${{ hashFiles('**/*.py') }}-${{ hashFiles('**/*.sql') }}-${{ hashFiles('**/*.cfg') }}

      - name: Build image and start airflow containers
        run: |
          docker-compose -f "airflow/docker-compose.yml" up --build -d

      - name: Wait for airflow webserver to be healthy
        run: |
          HEALTH=""
          while [ "$HEALTH" != "healthy" ]; do
            HEALTH=$(docker inspect --format='{{.State.Health.Status}}' airflow_webserver_1)
            if [ "$HEALTH" != "healthy" ]; then\
              echo "Waiting for airflow webserver to become healthy..."
              sleep 5
            fi
          done
      - name: Set up airflow connections
        run: |
          docker exec airflow_webserver_1 airflow connections add 'adzuna_conn' --conn-json '{"conn_type": "http", "extra": {"application_id": "${{ secrets.ADZUNA_CONN_APPLICATION_ID }}", "application_password": "${{ secrets.ADZUNA_CONN_APPLICATION_PASSWORD }}"}}'
          docker exec airflow_webserver_1 airflow connections add 'aws_custom' --conn-json '{"conn_type": "aws", "login":"${{ secrets.AWS_CUSTOM_LOGIN }}", "password":"${{ secrets.AWS_CUSTOM_PASSWORD }}", "extra": {"region_name": "ap-southeast-2"}}'
          docker exec airflow_webserver_1 airflow connections add 'openai_conn'  --conn-json '{"conn_type": "http", "password":"${{ secrets.OPENAI_CONN_PASSWORD }}"}'
          docker exec airflow_webserver_1 airflow connections add 'slack_conn' --conn-json '{"conn_type": "slack", "password":"${{ secrets.SLACK_CONN_PASSWORD }}"}'

      - name: Grant host machine access to EMR cluster
        run: |
          ip=$(curl ifconfig.me)
          echo $ip
          docker exec airflow_webserver_1 sh -c "cd CI && python3 -m update_security_group_rules ${{ env.EMR_MASTER_SECURITY_GROUP_ID }} $ip grant"

      - name: Run DAG validation and definition tests
        run: |
          docker exec airflow_webserver_1 python3 -m pytest -vv tests/

      - name: Run job_listing DAG
        run: |
          docker exec airflow_webserver_1 airflow dags unpause job_listing
          sleep 10

          job_listing_dag_run_status='queued'
          while [ "$job_listing_dag_run_status" == "running" -o "$job_listing_dag_run_status" == "queued" ]; do
            job_listing_dag_run_status=$(docker exec airflow_webserver_1 airflow dags list-runs -d job_listing | grep -oE 'running|queued|success|failed')
            echo "running job_listing"
            sleep 60
          done

          echo "job_listing DAG run "$job_listing_dag_run_status
          if [ "$job_listing_dag_run_status" == "failed" ]; then
            exit 1
          fi
      - name: Stop airflow containers
        run: |
          docker-compose -f "airflow/docker-compose.yml" down

      - name: Revoke host machine access to EMR cluster
        run: |
          ip=$(curl ifconfig.me)
          docker exec airflow_webserver_1 sh -c "cd CI && python3 -m update_security_group_rules ${{ env.EMR_MASTER_SECURITY_GROUP_ID }} $ip revoke"

      - name: set Docker directory permission for cache
        run: |
          sudo chmod -R 777 /var/lib/docker