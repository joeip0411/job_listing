name: CICD

on:
  pull_request:
    branches:
      - main
    paths:
      - 'airflow/**'
      - '.github/workflows/**'
  workflow_dispatch:

jobs:
  build_test:
    environment: dev
    name: Build and test image
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up .env file
        run: |
          touch .env
          echo  AIRFLOW_BASE_URL=${{ vars.AIRFLOW_BASE_URL }} >> .env
          echo  AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }} >> .env
          echo  AWS_DEFAULT_REGION=${{ vars.AWS_DEFAULT_REGION }} >> .env
          echo  AWS_REGION=${{ vars.AWS_REGION }} >> .env
          echo  AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }} >> .env
          echo  EC2_SUBNET_ID=${{ vars.EC2_SUBNET_ID}} >> .env
          echo  EMR_EC2_INSTANCE_PROFILE=${{ vars.EMR_EC2_INSTANCE_PROFILE}} >> .env
          echo  EMR_MASTER_SECURITY_GROUP_ID=${{ vars.EMR_MASTER_SECURITY_GROUP_ID }} >> .env
          echo  EMR_SERVICE_ROLE=${{ vars.EMR_SERVICE_ROLE }} >> .env
          echo  EMR_SLAVE_SECURITY_GROUP_ID=${{ vars.EMR_SLAVE_SECURITY_GROUP_ID }} >> .env
          echo  ENABLE_REMOTE_LOGGING=${{ vars.ENABLE_REMOTE_LOGGING }} >> .env
          echo  FERNET_KEY=${{ secrets.FERNET_KEY }} >> .env
          echo  LOG_BUCKET=${{ vars.LOG_BUCKET }} >> .env
          echo  POSTGRES_DB=${{ vars.POSTGRES_DB }} >> .env
          echo  POSTGRES_HOST=${{ vars.POSTGRES_HOST }} >> .env
          echo  POSTGRES_PORT=${{ vars.POSTGRES_PORT }} >> .env
          echo  POSTGRES_PASSWORD=${{ vars.POSTGRES_PASSWORD }} >> .env
          echo  POSTGRES_USER=${{ vars.POSTGRES_USER }} >> .env
          echo  PROD_GLUE_DATABASE=${{ vars.PROD_GLUE_DATABASE }} >> .env
          echo  GLUE_CATALOG=${{ vars.GLUE_CATALOG }} >> .env
          echo  GLUE_DATABASE=${{ vars.GLUE_DATABASE }} >> .env
          echo  GLUE_DATABASE_STORAGE_LOCATION=${{ vars.GLUE_DATABASE_STORAGE_LOCATION }} >> .env
          echo  STAGE=${{ vars.STAGE }} >> .env
          cat .env

      - name: Build image and start airflow containers
        run: |
          docker-compose up --build -d

      - name: Wait for airflow webserver to be healthy
        run: |
          HEALTH=""
          while [ "$HEALTH" != "healthy" ]; do
            HEALTH=$(docker inspect --format='{{.State.Health.Status}}' job_listing_webserver_1)
            if [ "$HEALTH" != "healthy" ]; then\
              echo "Waiting for airflow webserver to become healthy..."
              sleep 10
            fi
          done
      - name: Set up airflow connections
        run: |
          docker exec job_listing_webserver_1 airflow connections add 'adzuna_conn' --conn-json '{"conn_type": "http", "extra": {"application_id": "${{ secrets.ADZUNA_CONN_APPLICATION_ID }}", "application_password": "${{ secrets.ADZUNA_CONN_APPLICATION_PASSWORD }}"}}'
          docker exec job_listing_webserver_1 airflow connections add 'aws_custom' --conn-json '{"conn_type": "aws", "login":"${{ secrets.AWS_CUSTOM_LOGIN }}", "password":"${{ secrets.AWS_CUSTOM_PASSWORD }}", "extra": {"region_name": "${{ vars.AWS_DEFAULT_REGION }}"}}'
          docker exec job_listing_webserver_1 airflow connections add 'openai_conn'  --conn-json '{"conn_type": "http", "password":"${{ secrets.OPENAI_CONN_PASSWORD }}"}'
          docker exec job_listing_webserver_1 airflow connections add 'slack_conn' --conn-json '{"conn_type": "slack", "password":"${{ secrets.SLACK_CONN_PASSWORD }}"}'

      - name: Grant host machine access to EMR cluster
        run: |
          ip=$(curl ifconfig.me)
          echo $ip
          docker exec job_listing_webserver_1 sh -c "cd infra && python3 -m update_security_group_rules ${{ vars.EMR_MASTER_SECURITY_GROUP_ID }} $ip grant"

      - name: Set up database
        run: |
          docker exec job_listing_webserver_1 sh -c "cd infra && python3 -m glue_database_set_up"

      - name: Run DAG validation and definition tests
        run: |
          docker exec job_listing_webserver_1 python3 -m pytest -vv tests/

      - name: Run job_listing DAG
        run: |
          docker exec job_listing_webserver_1 airflow dags unpause job_listing
          sleep 10

          job_listing_dag_run_status='queued'
          while [ "$job_listing_dag_run_status" == "running" -o "$job_listing_dag_run_status" == "queued" ]; do
            job_listing_dag_run_status=$(docker exec job_listing_webserver_1 airflow dags list-runs -d job_listing | grep -oE 'running|queued|success|failed')
            echo "running job_listing"
            sleep 60
          done

          echo "job_listing DAG run "$job_listing_dag_run_status
          if [ "$job_listing_dag_run_status" == "failed" ]; then
            exit 1
          fi

      - name: Revoke host machine access to EMR cluster
        run: |
          ip=$(curl ifconfig.me)
          docker exec job_listing_webserver_1 sh -c "cd infra && python3 -m update_security_group_rules ${{ vars.EMR_MASTER_SECURITY_GROUP_ID }} $ip revoke"

      - name: Tear down database
        run: |
          docker exec job_listing_webserver_1 sh -c "cd infra && python3 -m glue_database_tear_down"

      - name: Stop airflow containers
        run: |
          docker-compose down

  deploy:
    needs: [build_test]
    environment: prod
    name: Deploy to ECR and ECS
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} 
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_DEFAULT_REGION }}

      - name: Login to AWS ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build, tag, and push image to Amazon ECR
        id: build-image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: airflow
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT
    
      - name: Fill in the new image ID in the Amazon ECS task definition
        id: render-airflow-webserver
        uses: aws-actions/amazon-ecs-render-task-definition@v1
        with:
          task-definition: infra/task-definition.json
          container-name: airflow-webserver
          image: ${{ steps.build-image.outputs.image }}
          environment-variables: |
            AIRFLOW_BASE_URL=${{ vars.AIRFLOW_BASE_URL }}
            AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}
            AWS_DEFAULT_REGION=${{ vars.AWS_DEFAULT_REGION }}
            AWS_REGION=${{ vars.AWS_REGION }}
            AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}
            ENABLE_REMOTE_LOGGING=${{ vars.ENABLE_REMOTE_LOGGING}}
            EC2_SUBNET_ID=${{ vars.EC2_SUBNET_ID}}
            EMR_EC2_INSTANCE_PROFILE=${{ vars.EMR_MASTER_SECURITY_GROUP_ID }}
            EMR_SERVICE_ROLE=${{ vars.EMR_SERVICE_ROLE }}
            EMR_SLAVE_SECURITY_GROUP_ID=${{ vars.EMR_SLAVE_SECURITY_GROUP_ID }}
            ENABLE_REMOTE_LOGGING=${{ vars.ENABLE_REMOTE_LOGGING }}
            FERNET_KEY=${{ secrets.FERNET_KEY }}
            LOG_BUCKET=${{ vars.LOG_BUCKET }}
            GLUE_CATALOG=${{ vars.GLUE_CATALOG}}
            GLUE_DATABASE=${{ vars.GLUE_DATABASE}}
            GLUE_DATABASE_STORAGE_LOCATION=${{ vars.GLUE_DATABASE_STORAGE_LOCATION}}
            POSTGRES_DB=${{ vars.POSTGRES_DB}}
            POSTGRES_HOST=${{ vars.POSTGRES_HOST}}
            POSTGRES_PASSWORD=${{ secrets.POSTGRES_PASSWORD}}
            POSTGRES_PORT=${{ vars.POSTGRES_PORT}}
            POSTGRES_USER=${{ vars.POSTGRES_USER}}
            STAGE=${{ vars.STAGE}}
          command: webserver

      - name: Modify Amazon ECS task definition with airflow-scheduler container
        id: render-airflow-scheduler
        uses: aws-actions/amazon-ecs-render-task-definition@v1
        with:
          task-definition: ${{ steps.render-airflow-webserver.outputs.task-definition }}
          container-name: airflow-scheduler
          image: ${{ steps.build-image.outputs.image }}
          environment-variables: |
            AIRFLOW_BASE_URL=${{ vars.AIRFLOW_BASE_URL }}
            AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}
            AWS_DEFAULT_REGION=${{ vars.AWS_DEFAULT_REGION }}
            AWS_REGION=${{ vars.AWS_REGION }}
            AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}
            ENABLE_REMOTE_LOGGING=${{ vars.ENABLE_REMOTE_LOGGING}}
            EC2_SUBNET_ID=${{ vars.EC2_SUBNET_ID}}
            EMR_EC2_INSTANCE_PROFILE=${{ vars.EMR_MASTER_SECURITY_GROUP_ID }}
            EMR_SERVICE_ROLE=${{ vars.EMR_SERVICE_ROLE }}
            EMR_SLAVE_SECURITY_GROUP_ID=${{ vars.EMR_SLAVE_SECURITY_GROUP_ID }}
            ENABLE_REMOTE_LOGGING=${{ vars.ENABLE_REMOTE_LOGGING }}
            FERNET_KEY=${{ secrets.FERNET_KEY }}
            LOG_BUCKET=${{ vars.LOG_BUCKET }}
            GLUE_CATALOG=${{ vars.GLUE_CATALOG}}
            GLUE_DATABASE=${{ vars.GLUE_DATABASE}}
            GLUE_DATABASE_STORAGE_LOCATION=${{ vars.GLUE_DATABASE_STORAGE_LOCATION}}
            POSTGRES_DB=${{ vars.POSTGRES_DB}}
            POSTGRES_HOST=${{ vars.POSTGRES_HOST}}
            POSTGRES_PASSWORD=${{ secrets.POSTGRES_PASSWORD}}
            POSTGRES_PORT=${{ vars.POSTGRES_PORT}}
            POSTGRES_USER=${{ vars.POSTGRES_USER}}
            STAGE=${{ vars.STAGE}}
          command: scheduler

      - name: Deploy ECS task definition to ECS service
        uses: aws-actions/amazon-ecs-deploy-task-definition@v1
        with:
          task-definition: ${{ steps.render-airflow-scheduler.outputs.task-definition }}
          service: ${{ vars.ECS_SERVICE_NAME}}
          cluster: ${{ vars.ECS_CLUSTER_NAME}}
          wait-for-service-stability: true